<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i" rel="stylesheet">
  <link href="/Tutorials/assets/css/syntax.css" rel="stylesheet" >
  <link href="/Tutorials/assets/css/tutorials.css" rel="stylesheet" >
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Seldonian Toolkit Tutorials | Learn how to build Seldonian algorithms</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Seldonian Toolkit Tutorials" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Learn how to build Seldonian algorithms" />
<meta property="og:description" content="Learn how to build Seldonian algorithms" />
<link rel="canonical" href="https://seldonian-toolkit.github.io/Tutorials/Tutorials/tutorials/install_toolkit_tutorial/" />
<meta property="og:url" content="https://seldonian-toolkit.github.io/Tutorials/Tutorials/tutorials/install_toolkit_tutorial/" />
<meta property="og:site_name" content="Seldonian Toolkit Tutorials" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Seldonian Toolkit Tutorials" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Learn how to build Seldonian algorithms","headline":"Seldonian Toolkit Tutorials","url":"https://seldonian-toolkit.github.io/Tutorials/Tutorials/tutorials/install_toolkit_tutorial/"}</script>
<!-- End Jekyll SEO tag -->

  <!-- Mathjax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>

<body class="tutorial">

  <!-- Navbar -->
    <nav class="navbar navbar-expand-lg navbar-dark" style="background-color: #0014A8;">
        <div class="container-fluid">
            <a class="navbar-brand" href="/Tutorials/">Seldonian ML</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/overview/">Overview</a>
                    </li>

                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                            Tutorials
                        </a>
                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/">Tutorials Home</a></li>
                            <li><hr class="dropdown-divider"></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/install_toolkit_tutorial/">1. Installing the Seldonian Toolkit</a></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/simple_engine_tutorial/">2. Getting started with the Seldonian Engine</a></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/fair_loans_tutorial/">3. Fairness in loan decision making</a></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/science_GPA_tutorial/">4. Reproducing Seldonian Science paper GPA classification results</a></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/custom_base_node_tutorial/">5. Creating your own constraint base variables</a></li>
                            <li><a class="dropdown-item" href="/Tutorials/tutorials/gridworld_RL_tutorial/">6. Reinforcement learning first tutorial: gridworld</a></li>
                        </ul>
                    </li>

                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                            Documentation
                        </a>
                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                            <li><a class="dropdown-item" href="/Tutorials/overview/">Overview</a></li>
                            <li><hr class="dropdown-divider"></li>
                            <li><a class="dropdown-item" href="https://seldonian-toolkit.github.io/Engine/build/html/index.html">Engine</a></li>
                            <li><a class="dropdown-item" href="https://seldonian-toolkit.github.io/Experiments/build/html/index.html">Experiments</a></li>
                            <li><a class="dropdown-item" href="https://seldonian-toolkit.github.io/GUI/build/html/index.html">GUI</a></li>
                        </ul>
                    </li>
            
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/glossary/">Glossary</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/faq/">FAQ</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/support/">Support</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/license/">License</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" aria-current="page" href="/Tutorials/about/">About</a>
                    </li>
        </div>
    </nav>
    <!-- .full-width here gets closed on support-page.html and home.html layouts -->


<div class="container my-4" align="center">
  
    <a href="/Tutorials/tutorials" class="btn btn-primary">&laquo; Previous: Tutorials home</a>
  
  
    <a href="/Tutorials/tutorials/simple_engine_tutorial/" class="btn btn-primary">Next: Simple Engine tutorial &raquo;</a>
  
</div>















<!-- Main Container -->
<div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
    <h2 align="center" class="mb-3">Tutorial: Seldonian algorithm details</h2>
    <hr class="my-4" />
    <h3>Outline</h3>
    <p>In this tutorial, you will learn:</p>
    <ul>
        <li>How the toolkit breaks a Seldonian algorithm into <code>candidate selection</code> and <code>safety test</code> components</li>
        <li>What the <code>safety test</code> does and how it works</li>
        <li>What <code>candidate selection</code> does and how it works</li>
        <li>How a parse tree enables users to specify complex behavioral constraints</li>
    </ul>
    Understanding these concepts will help you to understand how the toolkit works and how you can modify and improve it. <b>However, detailed understanding of these concepts is <i>not</i> required to use the toolkit.</b>
</div>

<div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
    <h3>Seldonian algorithm overview</h3>
    <p>
        A common misconception is that there is an algorithm that is <i>the</i> Seldonian algorithm. This is not the case&mdash;Seldonian algorithms are a class of algorithms, like classification or regression algorithms. Any algorithm \(a\) that ensures that for all \(i \in \{1,2,\dotsc,n\}\), \(\Pr(g_i(a(D))\leq 0)\geq 1-\delta_i\) is a Seldonian algorithm. For those not familiar with this expression or notation, we recommend first reviewing the <a href="https://seldonian.cs.umass.edu/Tutorials/overview/">overview page</a>.
    </p>
    <p>
        Though there are many different ways that a Seldonian algorithm can be designed, we have found one general algorithm structure that is often effective. This algorithm structure is depcted in the figure below.
    </p>
    <div class="container">
        <div class="row">
            <div class="col-md-2"></div>
            <div class="col-md-8">
                <img src="S15.png" class="img-fluid mx-auto d-block rounded shadow p-3 mb-5 bg-white" alt="Simple Seldonian Algorithm">
                <figcaption class="figure-caption"><p align="justify">Figure S15 (supplemental materials) P. S. Thomas, B. Castro da Silva, A. G. Barto, S. Giguere, Y. Brun, and E. Brunskill. Preventing undesirable behavior of intelligent machines. <em>Science</em>, 366:999&ndash;1004, 2019. Reprinted with permission from AAAS. A common misconception is that this algorithm is <i>the</i> Seldonian algorithm. There is no such thing, just as there is no one algorithm that is <i>the</i> reinforcement learning algorithm. This is one example of a Seldonian algorithm.</p></figcaption>
            </div>
            <div class="col-md-2"></div>
        </div>
    </div>
    <p>
        At a high level, Seldonian algorithms of this structure operate as follows. First, the available data \(D\) is partitioned into two sets, \(D_1\) and \(D_2\). After the publication of this figure in the original paper, we realized that it is not easy for people to remember which data set is \(D_1\) and which is \(D_2\). So, we have begun calling these the <i>safety data</i> \(D_\text{safety}=D_2\) and the <i>candidate data</i> \(D_\text{cand}=D_1\). The candidate data \(D_\text{cand}=D_1\) is provided as input to a component called <code>candidate selection</code> that selects a single solution that the algorithm plans to return. This solution is called the <i>candidate solution</i> \(\theta_c\). For now, you can imagine that <code>candidate selection</code> is your favorite off the shelf machine learning algorithm with no safety or fairness guarantees.
    </p>
    <p>
        Next, the candidate solution is provided as input to a component called <code>safety test</code> (or sometimes the <code>fairness test</code> for algorithms designed specifically for fairness applications). The <code>safety test</code> mechanism uses the safety data \(D_\text{safety}=D_2\) to determine whether the algorithm is sufficiently confident that \(\theta_c\) would be safe to return. If the safety test is sufficiently confident that \(\theta_c\) is safe, then it returns the candidate solution \(\theta_c\). Otherwise, it returns No Solution Found (NSF). The <code>safety test</code> component is responsible for ensuring that the algorithm is Seldonian.
    </p>
    <p>
        This algorithm structure should be natural, as it is what most data scientists would do if tasked with finding a machine learning model that is safe with high confidence. That is, they would train their model using some of the data (candidate selection), and would then use held-out data to verify that the model performs safely on unseen data (safety test). So, Seldonian algorithms essentially automate the exact process that a data scientist would follow.
    </p>
    <p>
        This toolkit implements Seldonian algorithms with this general structure. In the following sections we describe the safety test and candidate selection components in more detail. However, first we remind the reader that this algorithm structure is just one way that a Seldonian algorithm could be implemented. For example, the reinforcement learning algorithm presented in the <i>Science</i> paper performs the safety test <i>before</i> candidate selection.
    </p>
</div>

<div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
    <h3>Safety test</h3>
    <p>
            For simplicity, we begin by considering a simplified setting that limits the set of possible definitions of \(g_i\). Later in the "parse tree" section we show how this simplified setting can be extended to allow for more complicated definitions of \(g_i\). For now, we assume:
    </p>
    <ul>
        <li>The data \(D\) consists of \(m\) <i>independent and identically distributed</i> (i.i.d.) data points. In the supervised learning setting each data point is an input-label pair \((X_j,Y_j)\), while in the reinforcement learning setting each data point corresponds to an entire episode of data. Furthermore, let \(m_c\) and \(m_s\) be the sizes of the candidate and safety data sets respectively, so that \(m_c + m_s = m\). This assumption is not removed later.</li>
        <li>There is only a single behavioral constraint. That is, \(n=1\). We can therefore drop the \(i\) subscript and write \(\Pr(g(a(D))\leq 0)\geq 1-\delta\) rather than \(\Pr(g_i(a(D))\leq 0)\geq 1-\delta_i\).Later we describe the simple extension to enable multiple constraints.</li>
        <li>The user specifies the desired definition of \(g\) by providing a function \(\hat g\) that uses data to construct unbiased estimates of \(g(\theta)\). That is, \(g(\theta)=\mathbf{E}[\hat g(\theta,D)]\), where here \(D\) could be any subset of the available data (it could be \(D\), \(D_c\), \(D_s\), or any other set of i.i.d. data points). This is the key limiting assumption that is removed in the "parse tree" section below. However, many definitions of \(g\) do satisfy this constraint.</li>
    </ul>
    <p>
        The third assumption can be difficult to understand in this abstract form. To make it more concrete, consider an example: a regression problem where \(D=(X_j,Y_j)_{j=1}^m\), and where the goal is to minimize the mean squared error \(\operatorname{MSE}(\theta)=\mathbf{E}[(Y_j-\hat y(X_j,\theta))^2]\), where \(\hat y(X_j,\theta)\) is the regression model's prediction of \(Y_j\) based on the input \(X_j\) and using model paramters (weights) \(\theta\). Now imagine that the user of the algorithm wants to add in the safety constraint that \(\operatorname{MSE}(\theta) \leq 2.0\), i.e., that the model is sufficiently accurate when applied to future points not seen during training. This constraint corresponds to \(g(\theta) = \operatorname{MSE}(\theta) - 2.0\), since \(\theta\) is considered safe if and only if \(g(\theta) \leq 0\). In this case, the user could provide the function \(\hat g\) that returns one unbiased estimate of \(g(\theta)\) from each data point \((X_j,Y_j)\). This function would simply return the squared residual for the \(j^\text{th}\) point minus 2.0: \(\hat g(\theta,(X_j,Y_j)) = (Y_i - \hat y(X_j,\theta))^2 -2.0 \). If \(\hat g\) is provided with more than one point, it can return one unbiased estimate of \(g(\theta)\) for each provided point. 
    </p>
    <p>
        With these assumptions, it is relatively easy to create the <code>safety test</code> mechanism. First, the safety test passes the candidate solution \(\theta_c\) and the safety data \(D_\text{safety}=D_2\) to \(\hat g\). The result is an array of unbiased estimates of \(g(\theta_c)\). Let \(Z_1,\dotsc,Z_{m_s}\) be these unbiased estimates of \(g(\theta)\). Recall that since these estimates are i.i.d. and unbiased, we know that for any \(j\), \(\mathbf{E}[Z_j] = g(\theta)\). Next, the safety test constructs a \(1-\delta\) confidence upper bound on the expected value of \(Z_j\) using standard statistical tools like Hoeffding's inequality or Student's \(t\)-test. If this \(1-\delta\) confidence upper bound is at most zero, then the algorithm can conclude with confidence at least \(1-\delta\) that \(\theta_c\) is safe, and so it is returned. if the \(1-\delta\) confidence upper bound is greater than zero, the algorithm cannot conclude with sufficient confidence that \(\theta_c\) is safe, and so it returns No Solution Found (NSF) instead.
    </p>
    <p>
        In order to convert this English description into a precise mathematical statmenet for what the safety test does, we first review Student's \(t\)-test. 
    </p>
    <div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
        <h2>Student's \(t\)-test</h2>
        <p>
            Let \(Z_1,\dotsc,Z_m\) be \(m\) i.i.d. random variables and let \(\bar Z = \frac{1}{m}\sum_{i=1}^m Z_i\). If \(\bar Z\) is normally distributed, then for any \(\delta \in (0,1)\):
            $$\Pr\left (\mathbf{E}[Z_1] \leq \bar Z + \frac{\hat \sigma}{\sqrt{m}}t_{1-\delta,m-1}\right ) \geq 1-\delta,$$
            where \(\hat \sigma\) is the sample standard deviation including Bessel's correction:
            $$\hat \sigma = \sqrt{\frac{1}{m-1}\sum_{i=1}^m \left ( Z_i - \bar Z\right )^2},$$
            and where \(t_{1-\delta, \nu}\) is the \(100(1-\delta)\) percentile of the Student \(t\)-distribution with \(\nu\) degrees of freedom, i.e., <kbd>tinv\((1-\delta,\nu)\)</kbd> in <a href="https://www.mathworks.com/help/stats/tinv.html">Matlab</a>.
        </p>
        <p>
            To ground this abstract definition, consider an example. Imagine that we randomly selected \(m=30\) people from Earth (with replacement), and we measured their height in meters. Let these measurements be \(Z_1,\dotsc,Z_m\). These can be thought of as \(m\) i.i.d. samples of a random variable \(Z\) that corresponds to a randomly selected human's height. Student's \(t\)-test as described here would then provide a high-confidence upper bound on the average human height, \(\mathbf{E}[Z]\). However, it relies on the assumption that \(\frac{1}{m}\sum_{i=1}^m Z_i\) is normally distributed. This assumption is likely false, but is reasonable due to the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>. If the average height measurement is \(\bar Z=1.76m\) and the sample standard deviation of the measured heights is \(\hat \sigma=0.07m\), then we can apply Student's \(t\)-test to obtain a \(1-\delta\) confidence upper bound on the true (unknown because we only measured the heights of 30 people) average human height. Using \(\delta=0.1\) we obtain a \(0.9\)-confidence upper bound on the average human height of 
            $$
                \bar Z + \frac{\hat \sigma}{\sqrt{m}}t_{1-\delta,m-1} \approx 1.77m + \frac{0.07m}{\sqrt{30}}1.7 \approx 1.79m,
            $$
            where \(\approx\) is used when real numbers (like the value of \(t_{0.9,29}\)) are rounded. So, from this experiment, we could conclude with confidence \(0.9\) that the average human height is at most 1.79 meters.
        </p>
    </div>
    <p>
        Recall that in the safety we use apply Student's \(t\)-test to the outputs of \(\hat g(\theta_c, D_\text{safety})\) to obtain a \(1-\delta\)-confidence upper bound on \(g(\theta_c)\). Bringing together all the pieces, the safety test executes the following steps:
        <ol>
            <li>Compute \(\hat g(\theta_c, D_\text{safety})\), which produces as output unbiased estimates \(Z_1,\dotsc,Z_{m_s}\). Note: In general \(\hat g\) might output any number of unbiased estimates of \(g(\theta_c)\). In our example \(\hat g\) returns one estimate per data point, so here we use \(m_s\) (the number of points in the safety set) to denote the number of unbiased estimates produced by \(\hat g\). Also, recall that for our grounding example, \(Z_j\) corresponds to the squared residuals on the \(j^\text{th}\) data point minus \(2.0\).</li>
            <li>Compute the sample mean \(\bar Z = \frac{1}{m_s}\sum_{j=1}^{m_s}Z_j\).</li>
            <li>Compute the sample standard deviation \(\hat \sigma = \sqrt{\frac{1}{m-1}\sum_{i=1}^m \left ( Z_i - \bar Z\right )^2}\).</li>
            <li>
                Compute a \(1-\delta\)-confidence upper bound \(U\) on \(g(\theta_c)\) using Student's \(t\)-test. That is, \(U = \bar Z + \frac{\hat \sigma}{\sqrt{m}}t_{1-\delta,m-1}\).
            </li>
            <li>If \(U \leq 0\) return \(\theta_c\), otherwise return No Solution Found (NSF).</li>
        </ol>
    </p>
    <p>
        One nice property of the safety test is that it operates properly regardless of how \(\theta_c\) is chosen. This means that in theory <i>any</i> off the shelf machine learning algorithm could be used for the candidate selection component. However, in practice this would not be very effective if the candidate selection mechanism often returns solutions that are not safe (the algorithm would often return NSF). In the next section we discuss how the candidate selection mechanism can be designed to enable the algorithm to return NSF infrequently. 
    </p>
</div>

<div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
    <h3>Candidate selection</h3>
    <p>
        As discussed at the end of the previous section, any off the shelf machine learning algorithm could be used for the candidate selection component. However, most will tend to be ineffective. There are two issues. First, standard machine learning algorithms may not consider safety at all, and if they frequently return unsafe solutions the subsequent safety test will frequently return NSF. Second, even when standard machine learning algorithms do consider safety, they do not account for the details of the safety test that will be run. A more sophisticated candidate selection mechanism should consider the exact form of the subsequent safety test, and it should return candidate solutions that are likely to pass the safety test.
    </p>
    <p>
        In this toolkit, we provide a candidate selection mechanism that searches for the candidate solution \(\theta_c\) that optimizes a typical primary objective function (e.g., minimize classification loss, or maximize off-policy estimates of the expected return in the reinforcement learning setting) subject to the constraint that candidate selection <i>predicts</i> that the solution will pass the subsequent safety test. Candidate selection cannot actually compute whether or not the safety test will be passed because it does not have access to \(D_\text{safety}\). Using \(\hat f(\theta,D_\text{cand})\) to denote a primary objective function that should be maximized, we can then write an expression for the candidate solution:
        $$\theta_c \in \arg\max_{\theta \in \Theta}\hat f(\theta,D_\text{cand})\\\text{s.t. $\theta_c$ predicted to pass safety test}.$$ 
    </p>
    <p>
        This raises the question: how exactly should the safety test be predicted? This depends on the statistical tool used to compute the high-confidence upper bound on \(g(\theta_c)\). We use the following heuristic when using Student's \(t\)-test:
        $$\theta_c \in \arg\max_{\theta \in \Theta}\hat f(\theta,D_\text{cand})\\\text{s.t. }\forall i \in \{1,2,\dotsc,n\},\quad \bar Z + 2\frac{\hat \sigma}{\sqrt{|D_\text{safety}|}}t_{1-\delta_i,|D_\text{safety}|-1}\leq 0,$$
        where \(\bar Z\) and \(\hat \sigma\) are the sample mean and standard deviation of the unbiased estimates computed from the candidate data, i.e., the sample mean and standard deviation of \(g(\theta,D_\text{cand})\).
    </p>
    <p>
        We reiterate that this prediction of the safety test is a <i>heuristic</i> (one that we have found to be quite effective), not the result of a principled derivation. In particular, the constant \(2\) scaling the confidence interval from Student's \(t\)-test is chosen arbitrarily, and often other values work better in practice (in one case we found that a factor of \(3\) was more effective). We expect that this is one aspect of Seldonian algorithms that could be improved.
    </p>
    <p>
        The expression above describes the desired value of \(\theta_c\) as the solution to a constrained optimization problem. This raises the question: How should candidate selection compute or approximate the solution to this optimization problem? The Seldonian toolkit allows the user to select an optimizer. As a starting point, it includes the <a href="https://en.wikipedia.org/wiki/CMA-ES">CMA-ES</a> algorithm using a boundary function to incorporate the constraint. This is the gradient-free black-box optimizer that was used in several papers presenting Seldonian algorithms. Though it can be effective for small problems, it tends to be too slow for larger problems. To overcome this limitation, we also include a gradient-based optimizer.
    </p>
    <p>
        The gradient based optimizer uses gradient descent with adaptive step size schedules (<a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> by default). However, notice that we cannot directly use gradient ascent (or descent), because the problem is constrained. This can be overcome using the KKT conditions (a generalization of Lagrange Multipliers).
    </p>
    TODO - add box like t-test describing KKT.
    TODO - add conclusion giving the lagrangian in as much detail as we can (perhaps not much?) and saying that we use automatic differentiation to get the necessary derivatives.
</div>

<div class="container p-3 my-2 border" style="background-color: #f3f4fc;">
    <h3>Parse tree</h3>
    <p>
            TODO
    </p>
</div>



















    <footer>
      <!-- Bottom line to avoid spacing issues -->
      <div class="container p-3 my-5" align="center">
        
          <a href="/Tutorials/tutorials" class="btn btn-primary">&laquo; Previous: Tutorials home</a>
        
        
          <a href="/Tutorials/tutorials/simple_engine_tutorial/" class="btn btn-primary">Next: Simple Engine tutorial &raquo;</a>
        
      </div>
    </div>

    <hr class="my-4">
    </footer>


    <!-- Bootstrap JS Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>

    <!-- Script to run tooltips (information when hovering over links or buttons) -->
    <script>
        var tooltipTriggerList = [].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]'))
        var tooltipList = tooltipTriggerList.map(function (tooltipTriggerEl) {
        return new bootstrap.Tooltip(tooltipTriggerEl)
        })
    </script>

    <script>
    function copy2Clipboard(elem) {
      /* Get the text field */
      // var copyText = elem.parentNode;
      var copyText = elem.parentNode.children[1].textContent
       /* Copy the text inside the text field */
      navigator.clipboard.writeText(copyText);

      // /* Select the text field */
      // copyText.select();
      // copyText.setSelectionRange(0, 99999); /* For mobile devices */

      //  Copy the text inside the text field 
      // navigator.clipboard.writeText(copyText.value);
      
      // /* Alert the copied text */
      // alert("Copied the text: " + copyText.value);
    }
</script>
  </body>
</html>
<!-- Proudly powered by GitHub Pages ~ Generated 2022-08-17 02:42:51 +0000 -->

